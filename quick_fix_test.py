#!/usr/bin/env python3
"""
Quick test to verify if Ollama and the research system is working
"""

import asyncio
import requests
import json

async def test_ollama():
    """Test if Ollama is responding correctly"""
    print("ğŸ§ª Testing Ollama connection...")
    
    try:
        # Simple test request
        response = requests.post(
            "http://localhost:11434/api/chat",
            json={
                "model": "llama3.1:8b",
                "messages": [{"role": "user", "content": "Hello, respond with just 'OK'"}],
                "stream": False
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Ollama is responding correctly")
            message = result.get("message", {}).get("content", "No content")
            print(f"   Response: {message[:100]}...")
            return True
        else:
            print(f"âŒ Ollama error: {response.status_code}")
            print(f"   Response: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Ollama connection failed: {e}")
        return False

async def main():
    print("ğŸ”§ Quick Fix Test for Make It Heavy")
    print("=" * 40)
    
    # Test Ollama
    ollama_ok = await test_ollama()
    
    if ollama_ok:
        print("\nâœ… All systems operational!")
        print("ğŸš€ Your research should work now!")
        print("\nğŸ“‹ What was fixed:")
        print("   âœ… Supabase JSON parsing error")
        print("   âœ… Status error in research synthesis") 
        print("   âœ… Environment variables loaded")
        print("\nğŸ¯ Try running a research query again!")
    else:
        print("\nâš ï¸  Ollama needs attention:")
        print("   1. Make sure Ollama is running: ollama serve")
        print("   2. Check if model is available: ollama list")
        print("   3. Pull model if needed: ollama pull llama3.1:8b")

if __name__ == "__main__":
    asyncio.run(main())